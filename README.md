# LangGraph Document Service â€“ Documentation (v1.0)

---

## 1 Â· Overview

A microâ€‘service that ingests arbitrary documents (PDF, DOCX, TXT, â€¦) and exposes:

- **Semantic Q&A** (`GET /ask`)
- **Multiâ€‘document summarisation** (`GET /summary`)
- **Document catalogue & lifeâ€‘cycle management**

Key traits â–¶ï¸

- **FastAPI** REST API
- **LangChain + LangGraph** orchestration
- **OpenAI** GPTâ€‘4oâ€‘mini + textâ€‘embeddingâ€‘adaâ€‘002
- **Persistent vector store** â€“ ChromaDB (per document directory)
- **Relational metadata** â€“ SQLModel (SQLite / PostgreSQL)
- **Background ingestion** â€“ Celery + Redis

---

## 2 Â· Highâ€‘level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   1 HTTP POST    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  FastAPI / Gunicorn â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  (web pod)         â”‚
        â–²                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚2 status polls / queries            â”‚enqueue task
        â”‚                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  4 store metadata   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚    Celery worker   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  parse â†’ embed     â”‚
                                    â”‚  persist vectors   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  3 vector persist   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ ChromaDB dir â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Upload** â†’ API saves a *processing* record and enqueues `ingest_task`.
2. Client polls `/documents/{id}` until `status: ready`.
3. Worker writes embeddings to `vector_db/<doc_id>/` (Chroma) + `chunks.json`.
4. Worker updates DB (`status: ready`, `n_chunks`).

---

## 3 Â· Installation (local dev)

```bash
# clone repo
pip install -r requirements.txt  # or copy the list below
export OPENAI_API_KEY=skâ€‘â€¦
# optional: run postgres; fallback to SQLite works out of the box
redis-server &                    # needs Redis â‰¥ 6
uvicorn agent_app:app --reload    # web pod
celery -A agent_app.celery_app worker -l info  # worker pod
```

**Dependencies**

```
fastapi "uvicorn[standard]" langchain langchain-openai langgraph
sqlmodel sqlalchemy psycopg2-binary  # (or aiosqlite)
docling chromadb tiktoken numpy
celery redis
```

---

## 4 Â· Environment variables

| Var              | Default                    | Purpose                 |
| ---------------- | -------------------------- | ----------------------- |
| `OPENAI_API_KEY` | â€”                          | calls GPT & embeddings  |
| `DATABASE_URL`   | `sqlite:///./app.db`       | SQLModel engine         |
| `REDIS_URL`      | `redis://localhost:6379/0` | Celery broker / backend |

---

## 5 Â· Directory layout

```
.
â”œâ”€ agent_app.py          # FastAPI + Celery codebase (see canvas)
â”œâ”€ vector_db/            # one subâ€‘dir per document (Chroma index)
â”‚   â””â”€ <doc_id>/
â”‚       â”œâ”€ index.sqlite  # Chroma DB
â”‚       â””â”€ chunks.json   # raw chunk texts
â””â”€ app.db                # SQLite (if PostgreSQL not configured)
```

---

## 6 Â· API Reference

### 6.1 Upload Document

`POST /documents`

| Field                      | Type                | Notes                |
| -------------------------- | ------------------- | -------------------- |
| file (ğŸ—‹)                  | multipart/formâ€‘data | any supported format |
| **Returns** `202 Accepted` |                     |                      |

```json
{ "doc_id": "<uuid>", "status": "processing" }
```

### 6.2 List Documents

`GET /documents`

```json
[
  {"id":"â€¦","name":"report.pdf","status":"ready","n_chunks":28,"created_at":"â€¦"},
  â€¦
]
```

### 6.3 Document Detail / Status

`GET /documents/{id}` â†’ same schema as above.

### 6.4 Delete Document

`DELETE /documents/{id}` â†’ `{ "status": "deleted", "doc_id": "â€¦" }`

### 6.5 Multiâ€‘Document Summary

`GET /summary?doc_id=id1&doc_id=id2&length=medium&query=<topic>&top_k=10`

| Query                                 | Description                                                       |
| ------------------------------------- | ----------------------------------------------------------------- |
| `doc_id`                              | repeatable â€“ one or more UUIDs                                   |
| `length`                              | `short` â‰ˆ 3 sent., `medium` â‰ˆ 8, `long` â‰ˆ 15                      |
| `query` (optional)                    | topic/query to focus summary on using vector similarity search   |
| `top_k` (optional)                    | max relevant chunks when using query-focused mode (default: 10)  |
| **409** if any document not `ready`. |                                                                   |

**Query-Focused Summarization:** When `query` parameter is provided, the system performs vector similarity search to find the most relevant content chunks across all specified documents, then summarizes only those relevant portions. This enables focused summaries on specific topics rather than general document summaries.

**Multi-Topic Parallel Processing:** Use comma-separated topics (e.g., `query=machine learning,financial performance,project timeline`) to generate separate focused summaries for each topic processed in parallel using LangGraph's map-reduce pattern. Each topic gets its own vector similarity search and summary generation.

### 6.6 Semantic Q&A

`GET /ask?q=<question>&doc_id=<id>&doc_id=<id>&top_k=3`

- `q` â€“ freeâ€‘text question
- `top_k` â€“ retrieved chunks per document (default 3)

---

## 7 Â· Database Schema

```sql
CREATE TABLE doc (
  id TEXT PRIMARY KEY,
  name TEXT NOT NULL,
  status TEXT NOT NULL,       -- processing | ready | failed
  n_chunks INTEGER,
  created_at TIMESTAMP NOT NULL
);
```

---

## 8 Â· Background Ingestion Flow

1. **Celery task **``
   - parse â†’ `docling` â†’ Markdown
   - split â†’ `RecursiveCharacterTextSplitter`
   - embed & persist â†’ `Chroma.from_documents(..., persist_directory=â€¦)`
   - save chunks â†’ `chunks.json`
   - update `doc` row (`ready` + `n_chunks`)
2. Failure sets `status: failed` and keeps logs.

---

## 9 Â· How to Deploy

- **Docker / Compose** â€“ singleâ€‘host dev:
  - `api` image (FastAPI + Gunicorn)
  - `worker` image (Celery)
  - `redis`
  - `postgres`
- **Kubernetes** â€“ 2 deployments (`api`, `worker`) + 2 stateful services (`redis`, `postgres`).
- Mount `vector_db` on persistent volume claim (PVC) or switch to S3â€‘backed Chroma.

---

## 10 Â· Testing

```bash
pytest tests/            # unit + httpx integration tests
pytest -m e2e            # requires OPENAI_API_KEY set
```

Mock LLM via `langchain.chat_models.fake.FakeListChatModel` for CI.

---

## 11 Â· Extension Points

- Swap Chroma for Weaviate / Pinecone by replacing `langchain.vectorstores.Chroma` wrapper.
- Add topic segmentation endpoint (`/segments`) using embeddings + Kâ€‘Means.
- Enable SSE / WebSocket to stream token responses or ingest task progress.
- Implement OAuth / JWT to secure endpoints per user.

---

Â© 2025 LangGraph Doc Service â€¢ MIT

